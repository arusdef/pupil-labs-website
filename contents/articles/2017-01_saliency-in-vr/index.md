--- 
title: Saliency in VR
date: Mon Jan 09 2017 11:45:18 GMT+0700 (ICT) 
author: Pupil Dev Team 
subtitle: "Vincent et al., proposes new methods to learn and predict time-dependent saliency in VR without eye trackers..."
featured_img: "../../../../media/images/blog/vr_saliency.jpg"
featured_img_thumb: "../../../../media/images/blog/thumb/vr_saliency.jpg"
---

<img src="../../../../media/images/blog/vr_saliency.jpg" class='Feature-image u-padBottom--1' alt="">
Image Source: [Saliency in VR: How do people explore virtual environments?](https://arxiv.org/pdf/1612.04335.pdf)

The experiences offered by virtual environments differ from how we experience the real physical world and other forms of media. View behavior in immersive scenarios are much more complex than conventional displays due to the technology that allows interactions of kinematics that makes VR possible. 

To further understand the view behavior and saliency in VR, [Vincent Sitzmann]() et al., have collected a dataset that records gaze data and head orientation from users oberserving omni-directional stereo panoramas using VR head-mounted displays. 

The dataset reveals gaze and head interaction are connected together in VR, but do differ in some cases. Based on the data, Vincent and his colleagues proposes new methods to learn and predict time-dependent saliency in VR without eye trackers. The collected data may enable saliency models to be learned and if successful replace eye tracking that could approximate gaze movements from head and image information alone. 