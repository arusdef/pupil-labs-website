--- 
title: Saliency in VR
date: Mon Jan 09 2017 11:45:18 GMT+0700 (ICT) 
author: Pupil Dev Team 
subtitle: ""
featured_img: "../../../../media/images/blog/.jpg"
featured_img_thumb: "../../../../media/images/blog/thumb/.jpg"
---

<img src="../../../../media/images/blog/.jpg" class='Feature-image u-padBottom--1' alt="">
Image Source: []()

The experiences offered by virtual environments are different from how we experience the real physical world and other forms of media. View behavior in immersive scenarios are much more complex than conventional displays due to the technology that allows interactions of kinematics that makes VR possible. 

To further understand the view behavior and saliency in VR, [Vincent Sitzmann]() et al., have collected a dataset that records gaze data and head orientation from users oberserving omni-directional stereo panoramas using VR head-mounted displays. The dataset reveals patterns and biases where Vincent and his colleagues proposes new methods to learn and predict time-dependent saliency in VR without eye trackers.

The collected data may enable models to be learned that could approximate gaze movements from head and image information alone.
